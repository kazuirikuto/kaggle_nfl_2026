{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-18T11:50:52.074760Z",
     "iopub.status.busy": "2025-11-18T11:50:52.074437Z",
     "iopub.status.idle": "2025-11-18T13:07:58.888048Z",
     "shell.execute_reply": "2025-11-18T13:07:58.886942Z",
     "shell.execute_reply.started": "2025-11-18T11:50:52.074732Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Fitting Scaler/Imputer...\n",
      "Creating Sequences...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 149\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# ... (Your sequence creation code here) ...\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# For demonstration, assuming you have X_all, y_all, groups\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Note: 実際にはここで create_gnn_sequences を呼び出して X, y, groups を作成してください\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# X_all, y_all, groups = create_gnn_sequences(df, RAW_FEATURES, MAX_FRAMES, MAX_NODES)\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# --- 5-Fold Training ---\u001b[39;00m\n\u001b[1;32m    147\u001b[0m gkf \u001b[38;5;241m=\u001b[39m GroupKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (train_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gkf\u001b[38;5;241m.\u001b[39msplit(\u001b[43mX_all\u001b[49m, y_all, groups\u001b[38;5;241m=\u001b[39mgroups)):\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Training Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/5 ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    152\u001b[0m     X_train, y_train \u001b[38;5;241m=\u001b[39m X_all[train_idx], y_all[train_idx]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_all' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, Dense, TimeDistributed, Dropout, Reshape \n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.impute import SimpleImputer \n",
    "\n",
    "# --- 設定 ---\n",
    "RANDOM_STATE = 42\n",
    "# 【重要】ローカル環境のパスに合わせてください\n",
    "DATA_DIR = Path(\"/home/rikuto/kaggle_note/competitions/nfl-big-data-bowl-2026-prediction/data/raw\")\n",
    "OUTPUT_DIR = Path(\"/home/rikuto/kaggle_note/competitions/nfl-big-data-bowl-2026-prediction/notebooks/artifacts\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# --- グローバル変数 ---\n",
    "RAW_FEATURES = [\n",
    "    'track_x', 'track_y', 's', 'a', \n",
    "    'dir_sin', 'dir_cos', 'o_sin', 'o_cos',\n",
    "    'player_bmi', 'player_age_years', \n",
    "    'is_defense', 'is_offense', 'is_ball' \n",
    "]\n",
    "TARGET_COLS = ['target_x', 'target_y']\n",
    "MAX_FRAMES = 100      \n",
    "MAX_NODES = 23        \n",
    "PADDING_VALUE = 0.0   \n",
    "TRAIN_WEEKS = [f\"{week:02d}\" for week in range(1, 19)]\n",
    "TARGET_KEYS = [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"] \n",
    "SELECT_COLUMNS = [\n",
    "    \"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"player_to_predict\",\n",
    "    \"player_height\", \"player_weight\", \"player_birth_date\", \"player_position\",\n",
    "    \"player_side\", \"player_role\", \"x\", \"y\", \"s\", \"a\", \"dir\", \"o\",\n",
    "    \"num_frames_output\"\n",
    "]\n",
    "\n",
    "# --- ヘルパー関数 ---\n",
    "def _parse_height(heights: pd.Series) -> pd.Series:\n",
    "    parts = heights.fillna(\"0-0\").str.split(\"-\", expand=True)\n",
    "    feet = pd.to_numeric(parts[0], errors=\"coerce\")\n",
    "    inches = pd.to_numeric(parts[1], errors=\"coerce\")\n",
    "    return feet * 12 + inches\n",
    "\n",
    "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    data = df.copy()\n",
    "    data[\"player_position\"] = data[\"player_position\"].fillna(\"unknown\")\n",
    "    data[\"player_side\"] = data[\"player_side\"].fillna(\"unknown\")\n",
    "    data[\"player_role\"] = data[\"player_role\"].fillna(\"unknown\")\n",
    "\n",
    "    data[\"height_inches\"] = _parse_height(data.get(\"player_height\", pd.Series(index=data.index, dtype=str)))\n",
    "    data[\"player_weight\"] = pd.to_numeric(data.get(\"player_weight\"), errors=\"coerce\")\n",
    "    height_m = data[\"height_inches\"] * 0.0254\n",
    "    weight_kg = data[\"player_weight\"] * 0.45359237\n",
    "    data[\"player_bmi\"] = weight_kg / np.square(height_m)\n",
    "    data.loc[~np.isfinite(data[\"player_bmi\"]), \"player_bmi\"] = np.nan\n",
    "\n",
    "    birth_dates = pd.to_datetime(data.get(\"player_birth_date\"), errors=\"coerce\")\n",
    "    game_dates = pd.to_datetime(data[\"game_id\"].astype(str).str[:8], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "    data[\"player_age_years\"] = (game_dates - birth_dates).dt.days / 365.25\n",
    "    \n",
    "    data[\"is_defense\"] = (data[\"player_side\"].str.lower() == \"defense\").astype(\"int8\")\n",
    "    data[\"is_offense\"] = (data[\"player_side\"].str.lower() == \"offense\").astype(\"int8\")\n",
    "    data[\"is_ball\"] = (data[\"player_role\"].str.lower() == \"football\").astype(\"int8\")\n",
    "\n",
    "    data = data.rename(columns={\"x\": \"track_x\", \"y\": \"track_y\"}, errors=\"ignore\")\n",
    "    \n",
    "    for angle_col in (\"dir\", \"o\"):\n",
    "        if angle_col in data.columns:\n",
    "            radians = np.deg2rad(data[angle_col].fillna(0)) \n",
    "            data[f\"{angle_col}_sin\"] = np.sin(radians)\n",
    "            data[f\"{angle_col}_cos\"] = np.cos(radians)\n",
    "            data.loc[data[angle_col].isna(), [f\"{angle_col}_sin\", f\"{angle_col}_cos\"]] = np.nan\n",
    "        else:\n",
    "            data[f\"{angle_col}_sin\"] = np.nan\n",
    "            data[f\"{angle_col}_cos\"] = np.nan\n",
    "            \n",
    "    return data\n",
    "\n",
    "def load_week(week: str) -> pd.DataFrame:\n",
    "    input_path = DATA_DIR / \"train\" / f\"input_2023_w{week}.csv\"\n",
    "    output_path = DATA_DIR / \"train\" / f\"output_2023_w{week}.csv\"\n",
    "    \n",
    "    use_cols = SELECT_COLUMNS.copy()\n",
    "    features = pd.read_csv(input_path, usecols=use_cols)\n",
    "    targets = pd.read_csv(output_path).rename(columns={\"x\": \"target_x\", \"y\": \"target_y\"})\n",
    "    \n",
    "    features = features.merge(targets, on=TARGET_KEYS, how=\"left\", suffixes=('', '_target'))\n",
    "    print(f\"Week {week}: {len(features):,} rows loaded.\")\n",
    "    return features\n",
    "\n",
    "def load_training_data() -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for week in TRAIN_WEEKS:\n",
    "        frames.append(load_week(week))\n",
    "    train_df = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    print(\"Running feature engineering...\")\n",
    "    train_df = engineer_features(train_df)\n",
    "    \n",
    "    train_df['is_target_player'] = (\n",
    "        train_df['player_to_predict'] & \n",
    "        (train_df['frame_id'] <= train_df['num_frames_output'])\n",
    "    )\n",
    "    return train_df\n",
    "\n",
    "# --- シーケンス作成関数 (これが抜けていました) ---\n",
    "def create_gnn_sequences(df: pd.DataFrame, feature_cols: list[str], max_frames: int, max_nodes: int) -> tuple:\n",
    "    df['play_seq_id'] = df['game_id'].astype(str) + \"_\" + df['play_id'].astype(str)\n",
    "    \n",
    "    print(\"Grouping by play and padding...\")\n",
    "    grouped_plays = df.groupby('play_seq_id')\n",
    "    \n",
    "    X_plays, y_plays = [], []\n",
    "    groups_cv = [] \n",
    "    \n",
    "    play_ids = df['play_seq_id'].unique()\n",
    "    \n",
    "    for play_id in play_ids:\n",
    "        play_df = grouped_plays.get_group(play_id)\n",
    "        grouped_frames = play_df.groupby('frame_id')\n",
    "        X_frames, y_frames = [], []\n",
    "        frame_ids = sorted(play_df['frame_id'].unique())\n",
    "        \n",
    "        for frame_id in frame_ids:\n",
    "            frame_df = grouped_frames.get_group(frame_id)\n",
    "            \n",
    "            # Features\n",
    "            frame_X = frame_df[feature_cols].values\n",
    "            pad_width_X = ((0, max_nodes - len(frame_X)), (0, 0))\n",
    "            frame_X_padded = np.pad(frame_X, pad_width_X, 'constant', constant_values=PADDING_VALUE)\n",
    "            X_frames.append(frame_X_padded)\n",
    "            \n",
    "            # Targets\n",
    "            frame_y_targets = frame_df[['target_x', 'target_y']].values\n",
    "            frame_mask = frame_df['is_target_player'].values\n",
    "            frame_y = np.where(frame_mask[:, None], frame_y_targets, np.nan) \n",
    "            \n",
    "            pad_width_y = ((0, max_nodes - len(frame_y)), (0, 0))\n",
    "            frame_y_padded = np.pad(frame_y, pad_width_y, 'constant', constant_values=np.nan)\n",
    "            y_frames.append(frame_y_padded)\n",
    "        \n",
    "        # Cutoff or Pad frames\n",
    "        if len(X_frames) > max_frames:\n",
    "            X_frames = X_frames[:max_frames]\n",
    "            y_frames = y_frames[:max_frames]\n",
    "        \n",
    "        pad_len = max(0, max_frames - len(X_frames))\n",
    "        pad_width_frames = ((0, pad_len), (0, 0), (0, 0))\n",
    "        \n",
    "        X_play_padded = np.pad(X_frames, pad_width_frames, 'constant', constant_values=PADDING_VALUE)\n",
    "        y_play_padded = np.pad(y_frames, pad_width_frames, 'constant', constant_values=np.nan)\n",
    "        y_play_padded = np.nan_to_num(y_play_padded, nan=PADDING_VALUE) \n",
    "        \n",
    "        X_plays.append(X_play_padded)\n",
    "        y_plays.append(y_play_padded)\n",
    "        groups_cv.append(play_id) \n",
    "        \n",
    "    return (\n",
    "        np.array(X_plays, dtype='float32'),\n",
    "        np.array(y_plays, dtype='float32'),\n",
    "        np.array(groups_cv)\n",
    "    )\n",
    "\n",
    "# --- モデル定義 ---\n",
    "def masked_mae_loss(y_true, y_pred):\n",
    "    mask = tf.cast(tf.not_equal(y_true, PADDING_VALUE), tf.float32)\n",
    "    abs_error = tf.abs(y_true - y_pred)\n",
    "    masked_error = abs_error * mask\n",
    "    total_error = tf.reduce_sum(masked_error)\n",
    "    num_non_zero = tf.reduce_sum(mask)\n",
    "    return tf.math.divide_no_nan(total_error, num_non_zero)\n",
    "\n",
    "def build_model(num_features):\n",
    "    X_in = Input(shape=(MAX_FRAMES, MAX_NODES, num_features))\n",
    "    x = TimeDistributed(TimeDistributed(Dense(32, activation='relu')))(X_in)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Reshape((MAX_FRAMES * MAX_NODES, 32))(x)\n",
    "    x = LSTM(64, return_sequences=True)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Reshape((MAX_FRAMES, MAX_NODES, 64))(x)\n",
    "    output = TimeDistributed(TimeDistributed(Dense(2)))(x)\n",
    "    model = Model(inputs=X_in, outputs=output)\n",
    "    model.compile(optimizer=Adam(0.001), loss=masked_mae_loss)\n",
    "    return model\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading Data...\")\n",
    "    df = load_training_data()\n",
    "    \n",
    "    print(\"Fitting Scaler/Imputer...\")\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    df[RAW_FEATURES] = imputer.fit_transform(df[RAW_FEATURES])\n",
    "    df[RAW_FEATURES] = scaler.fit_transform(df[RAW_FEATURES])\n",
    "    \n",
    "    # Save preprocessors\n",
    "    print(f\"Saving preprocessors to {OUTPUT_DIR}...\")\n",
    "    joblib.dump(imputer, OUTPUT_DIR / 'imputer.pkl')\n",
    "    joblib.dump(scaler, OUTPUT_DIR / 'scaler.pkl')\n",
    "    \n",
    "    # シーケンス作成 (ここでX_all, y_allを作成)\n",
    "    print(\"Creating Sequences...\")\n",
    "    X_all, y_all, groups = create_gnn_sequences(df, RAW_FEATURES, MAX_FRAMES, MAX_NODES)\n",
    "    \n",
    "    print(f\"X shape: {X_all.shape}, y shape: {y_all.shape}\")\n",
    "    \n",
    "    # --- 5-Fold Training ---\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X_all, y_all, groups=groups)):\n",
    "        print(f\"\\n--- Training Fold {fold + 1}/5 ---\")\n",
    "        \n",
    "        X_train, y_train = X_all[train_idx], y_all[train_idx]\n",
    "        X_val, y_val = X_all[val_idx], y_all[val_idx]\n",
    "        \n",
    "        model = build_model(len(RAW_FEATURES))\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=20, \n",
    "            batch_size=32,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # モデル保存\n",
    "        save_path = OUTPUT_DIR / f'model_fold_{fold}.keras'\n",
    "        model.save(save_path)\n",
    "        print(f\"Saved: {save_path}\")\n",
    "        \n",
    "        # メモリ解放\n",
    "        del model, X_train, y_train, X_val, y_val\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "    print(\"\\nAll folds trained successfully.\")\n",
    "    print(f\"Upload the '{OUTPUT_DIR}' folder content to a Kaggle Dataset.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14210809,
     "sourceId": 114239,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "nfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
